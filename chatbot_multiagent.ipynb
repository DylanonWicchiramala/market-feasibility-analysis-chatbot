{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "\n",
    "utils.load_env()\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "set_verbose(True)\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def store_memory(result, user_id):\n",
    "#      history.update_one({\"user_id\":user_id}, {\"$push\":{\n",
    "#           \"chat_history\" : {\"$each\":[\n",
    "#                          (result[\"messages\"][0].content),\n",
    "#                          (result[\"messages\"][-1].content)\n",
    "#                             ]}}})\n",
    "\n",
    "# def QA(question, user_id):\n",
    "#      query = history.find_one({\"user_id\": user_id})\n",
    "#      if query is None:\n",
    "#           query = {\n",
    "#                \"user_id\": user_id,\n",
    "#                \"chat_history\": [],\n",
    "#           }\n",
    "#           history.insert_one(query)\n",
    "     \n",
    "#      chat_history = []\n",
    "#      for i, msg in enumerate(query[\"chat_history\"]):\n",
    "#           chat_history.append(\n",
    "#                AIMessage(msg) if i % 2 == 1 else HumanMessage(msg)\n",
    "#           )\n",
    "\n",
    "#      result = graph.invoke({\n",
    "#           \"messages\": [\n",
    "#                HumanMessage(\n",
    "#                     content=question\n",
    "#                )\n",
    "#           ],\n",
    "#           \"chat_history\":chat_history,\n",
    "          \n",
    "#      },)\n",
    "#      store_memory(result, user_id)\n",
    "#      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain_core.messages import HumanMessage\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# for llm model\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from tools import (\n",
    "    find_place_from_text, \n",
    "    nearby_search, \n",
    "    nearby_dense_community, \n",
    "    google_search, \n",
    "    population_doc_retriever,\n",
    "    duckduckgo_search\n",
    ")\n",
    "from typing import Annotated, Sequence, TypedDict, List\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, \n",
    "    HumanMessage,\n",
    "    BaseMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "\n",
    "## tools and LLM\n",
    "# Bind the tools to the model\n",
    "tools = [population_doc_retriever, find_place_from_text, nearby_search, nearby_dense_community, duckduckgo_search]  # Include both tools if needed\n",
    "# tools = [google_search]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "\n",
    "## Create agents\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    # memory = ConversationBufferMemory(memory_key='chat_history', return_messages=False)\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" \"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    #llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    # agent = LLMChain(\n",
    "    #     prompt=prompt,\n",
    "    #     llm=llm_with_tools,\n",
    "    #     # memory=memory\n",
    "    # )\n",
    "    # return agent\n",
    "    return prompt | llm.bind_tools(tools)\n",
    "    #agent = prompt | llm_with_tools\n",
    "    #return agent\n",
    "\n",
    "\n",
    "## Define state\n",
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    chat_history: List[BaseMessage]\n",
    "    sender: str\n",
    "\n",
    "\n",
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "        # result = AIMessage(**result.dict(), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "\n",
    "## Define Agents Node\n",
    "# Research agent and node\n",
    "from prompt import agent_meta\n",
    "agent_name = [meta['name'] for meta in agent_meta]\n",
    "\n",
    "agents={}\n",
    "agent_nodes={}\n",
    "\n",
    "for meta in agent_meta:\n",
    "    name = meta['name']\n",
    "    prompt = meta['prompt']\n",
    "    \n",
    "    agents[name] = create_agent(\n",
    "            llm,\n",
    "            tools,\n",
    "            system_message=prompt,\n",
    "        )\n",
    "    \n",
    "    agent_nodes[name] = functools.partial(agent_node, agent=agents[name], name=name)\n",
    "\n",
    "\n",
    "## Define Tool Node\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Literal\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def router(state) -> Literal[\"call_tool\", \"__end__\", \"continue\"]:\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"continue\" in last_message.content:\n",
    "        return \"continue\"\n",
    "    if last_message.tool_calls:\n",
    "        # The previous agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    if \"%SIjfE923hf\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "\n",
    "## Workflow Graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# add agent nodes\n",
    "for name, node in agent_nodes.items():\n",
    "    workflow.add_node(name, node)\n",
    "    \n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyst\",\n",
    "    router,\n",
    "    {\"continue\": \"data_collector\", \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"data_collector\",\n",
    "    router,\n",
    "    {\"call_tool\": \"call_tool\", \"continue\": \"reporter\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"reporter\",\n",
    "    router,\n",
    "    {\"continue\": \"data_collector\", \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Each agent node updates the 'sender' field\n",
    "    # the tool calling node does not, meaning\n",
    "    # this edge will route back to the original agent\n",
    "    # who invoked the tool\n",
    "    lambda x: x[\"sender\"],\n",
    "    {name:name for name in agent_name},\n",
    ")\n",
    "workflow.add_edge(START, \"analyst\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"ค้นหาร้านของชำใกล้อนุสาวรีย์ชัยฯ พร้อมวิเคราะห์จำนวนประชากร\"\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                question\n",
    "            )\n",
    "        ],\n",
    "        \"chat_history\": [    \n",
    "        ]\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 50},\n",
    "    debug=False\n",
    ")\n",
    "for s in events:\n",
    "    # print(s)\n",
    "    a = list(s.items())[0]\n",
    "    a[1]['messages'][0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "def submitUserMessage(user_input: str) -> str:\n",
    "    graph = workflow.compile()\n",
    "\n",
    "    events = graph.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    user_input\n",
    "                )\n",
    "            ],\n",
    "            \"chat_history\": [    \n",
    "            ]\n",
    "        },\n",
    "        # Maximum number of steps to take in the graph\n",
    "        {\"recursion_limit\": 20},\n",
    "    )\n",
    "    \n",
    "    events = [e for e in events]\n",
    "    \n",
    "    response = list(events[-1].values())[0][\"messages\"][0]\n",
    "    response = response.content\n",
    "    response = response.replace(\"%SIjfE923hf\", \"\")\n",
    "    \n",
    "    chat_history.append(HumanMessage(user_input))\n",
    "    chat_history.append(AIMessage(response))\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# question = \"hello my frend\"\n",
    "# submitUserMessage(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
