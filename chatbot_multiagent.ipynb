{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def store_memory(result, user_id):\n",
    "#      history.update_one({\"user_id\":user_id}, {\"$push\":{\n",
    "#           \"chat_history\" : {\"$each\":[\n",
    "#                          (result[\"messages\"][0].content),\n",
    "#                          (result[\"messages\"][-1].content)\n",
    "#                             ]}}})\n",
    "\n",
    "# def QA(question, user_id):\n",
    "#      query = history.find_one({\"user_id\": user_id})\n",
    "#      if query is None:\n",
    "#           query = {\n",
    "#                \"user_id\": user_id,\n",
    "#                \"chat_history\": [],\n",
    "#           }\n",
    "#           history.insert_one(query)\n",
    "     \n",
    "#      chat_history = []\n",
    "#      for i, msg in enumerate(query[\"chat_history\"]):\n",
    "#           chat_history.append(\n",
    "#                AIMessage(msg) if i % 2 == 1 else HumanMessage(msg)\n",
    "#           )\n",
    "\n",
    "#      result = graph.invoke({\n",
    "#           \"messages\": [\n",
    "#                HumanMessage(\n",
    "#                     content=question\n",
    "#                )\n",
    "#           ],\n",
    "#           \"chat_history\":chat_history,\n",
    "          \n",
    "#      },)\n",
    "#      store_memory(result, user_id)\n",
    "#      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env ------------------------------------------------------------------------\n",
    "import os\n",
    "import utils\n",
    "\n",
    "utils.load_env()\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"false\"\n",
    "\n",
    "\n",
    "# debug ------------------------------------------------------------------\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "\n",
    "set_verbose(True)\n",
    "set_debug(False)\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# for llm model\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from tools import (\n",
    "    find_place_from_text, \n",
    "    nearby_search, \n",
    "    nearby_dense_community, \n",
    "    google_search, \n",
    "    population_doc_retriever,\n",
    "    duckduckgo_search\n",
    ")\n",
    "from typing import Annotated, Sequence, TypedDict, List\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, \n",
    "    HumanMessage,\n",
    "    BaseMessage,\n",
    "    ToolMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "\n",
    "## tools and LLM\n",
    "# Bind the tools to the model\n",
    "tools = [population_doc_retriever, find_place_from_text, nearby_search, nearby_dense_community, duckduckgo_search]  # Include both tools if needed\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "\n",
    "## Create agents ------------------------------------------------------------------------\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    # memory = ConversationBufferMemory(memory_key='chat_history', return_messages=False)\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" \"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    #llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    agent = prompt | llm_with_tools\n",
    "    return agent\n",
    "\n",
    "\n",
    "## Define state ------------------------------------------------------------------------\n",
    "# This defines the object that is passed between each node\n",
    "# in the graph. We will create different nodes for each agent and tool\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    chat_history: List[BaseMessage]\n",
    "    sender: str\n",
    "\n",
    "\n",
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # We convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "        # result = AIMessage(**result.dict(), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        # Since we have a strict workflow, we can\n",
    "        # track the sender so we know who to pass to next.\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "\n",
    "## Define Agents Node ------------------------------------------------------------------------\n",
    "# Research agent and node\n",
    "from prompt import agent_meta\n",
    "agent_name = [meta['name'] for meta in agent_meta]\n",
    "\n",
    "agents={}\n",
    "agent_nodes={}\n",
    "\n",
    "for meta in agent_meta:\n",
    "    name = meta['name']\n",
    "    prompt = meta['prompt']\n",
    "    \n",
    "    agents[name] = create_agent(\n",
    "            llm,\n",
    "            tools,\n",
    "            system_message=prompt,\n",
    "        )\n",
    "    \n",
    "    agent_nodes[name] = functools.partial(agent_node, agent=agents[name], name=name)\n",
    "\n",
    "\n",
    "## Define Tool Node\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Literal\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def router(state) -> Literal[\"call_tool\", \"__end__\", \"continue\"]:\n",
    "    # This is the router\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if \"continue\" in last_message.content:\n",
    "        return \"continue\"\n",
    "    if last_message.tool_calls:\n",
    "        # The previous agent is invoking a tool\n",
    "        return \"call_tool\"\n",
    "    if \"%SIjfE923hf\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "\n",
    "## Workflow Graph ------------------------------------------------------------------------\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# add agent nodes\n",
    "for name, node in agent_nodes.items():\n",
    "    workflow.add_node(name, node)\n",
    "    \n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyst\",\n",
    "    router,\n",
    "    {\"continue\": \"data_collector\", \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"data_collector\",\n",
    "    router,\n",
    "    {\"call_tool\": \"call_tool\", \"continue\": \"reporter\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"reporter\",\n",
    "    router,\n",
    "    {\"continue\": \"data_collector\", \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Each agent node updates the 'sender' field\n",
    "    # the tool calling node does not, meaning\n",
    "    # this edge will route back to the original agent\n",
    "    # who invoked the tool\n",
    "    lambda x: x[\"sender\"],\n",
    "    {name:name for name in agent_name},\n",
    ")\n",
    "\n",
    "workflow.add_edge(START, \"analyst\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: analyst\n",
      "Tool Calls:\n",
      "  find_place_from_text (call_9sVh1jL4mO4b2vduoIms1WDs)\n",
      " Call ID: call_9sVh1jL4mO4b2vduoIms1WDs\n",
      "  Args:\n",
      "    location: นวลจันทร์\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: find_place_from_text\n",
      "\n",
      "\n",
      "    # address: นวลจันทร์ กรุงเทพมหานคร 10230 ประเทศไทย\n",
      "\n",
      "    location_name: นวลจันทร์\n",
      "\n",
      "    \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: analyst\n",
      "Tool Calls:\n",
      "  search_population_community_household_expenditures_data (call_7UbwNalXXbW65TCEGjkabumG)\n",
      " Call ID: call_7UbwNalXXbW65TCEGjkabumG\n",
      "  Args:\n",
      "    query: นวลจันทร์\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_population_community_household_expenditures_data\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ยานนาวา\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ปทุมวัน\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ทวีวัฒนา\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: คันนายาว\n",
      ": 42\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: analyst\n",
      "\n",
      "%SIjfE923hf การเปิดร้านอาหารแถวนวลจันทร์สามารถพิจารณาจากข้อมูลประชากรและชุมชนในพื้นที่ โดยในกรุงเทพมหานครมีจำนวนชุมชนประเภทต่าง ๆ ที่แตกต่างกันไป เช่น ในเขตคันนายาวมีจำนวนชุมชนถึง 42 ชุมชน ซึ่งอาจแสดงถึงความหนาแน่นของประชากรและโอกาสในการทำธุรกิจร้านอาหาร\n",
      "\n",
      "การวิเคราะห์เพิ่มเติมเกี่ยวกับค่าใช้จ่ายของครัวเรือนในพื้นที่นวลจันทร์จะช่วยให้คุณเข้าใจถึงกำลังซื้อของลูกค้าในพื้นที่นี้ได้ดียิ่งขึ้น อย่างไรก็ตาม ข้อมูลเฉพาะเกี่ยวกับค่าใช้จ่ายในนวลจันทร์ยังไม่มีให้ในขณะนี้\n",
      "\n",
      "หากคุณต้องการข้อมูลเพิ่มเติมเกี่ยวกับค่าใช้จ่ายหรือประชากรในพื้นที่นี้ กรุณาแจ้งให้ทราบเพื่อให้สามารถช่วยเหลือได้มากขึ้น\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' การเปิดร้านอาหารแถวนวลจันทร์สามารถพิจารณาจากข้อมูลประชากรและชุมชนในพื้นที่ โดยในกรุงเทพมหานครมีจำนวนชุมชนประเภทต่าง ๆ ที่แตกต่างกันไป เช่น ในเขตคันนายาวมีจำนวนชุมชนถึง 42 ชุมชน ซึ่งอาจแสดงถึงความหนาแน่นของประชากรและโอกาสในการทำธุรกิจร้านอาหาร\\n\\nการวิเคราะห์เพิ่มเติมเกี่ยวกับค่าใช้จ่ายของครัวเรือนในพื้นที่นวลจันทร์จะช่วยให้คุณเข้าใจถึงกำลังซื้อของลูกค้าในพื้นที่นี้ได้ดียิ่งขึ้น อย่างไรก็ตาม ข้อมูลเฉพาะเกี่ยวกับค่าใช้จ่ายในนวลจันทร์ยังไม่มีให้ในขณะนี้\\n\\nหากคุณต้องการข้อมูลเพิ่มเติมเกี่ยวกับค่าใช้จ่ายหรือประชากรในพื้นที่นี้ กรุณาแจ้งให้ทราบเพื่อให้สามารถช่วยเหลือได้มากขึ้น'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history=[]\n",
    "def submitUserMessage_debug(user_input: str, keep_chat_history:bool=True) -> str:\n",
    "    global chat_history\n",
    "    chat_history.append(HumanMessage(user_input))\n",
    "    \n",
    "    if not keep_chat_history:\n",
    "        chat_history = []\n",
    "    graph = workflow.compile()\n",
    "\n",
    "    events = graph.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    user_input\n",
    "                )\n",
    "            ],\n",
    "            \"chat_history\": chat_history\n",
    "        },\n",
    "        # Maximum number of steps to take in the graph\n",
    "        {\"recursion_limit\": 20},\n",
    "    )\n",
    "    \n",
    "    for s in events:\n",
    "        # print(s)\n",
    "        a = list(s.items())[0]\n",
    "        a[1]['messages'][0].pretty_print()\n",
    "    \n",
    "    response = a[1]['messages'][0].content\n",
    "    response = response.replace(\"%SIjfE923hf\", \"\")\n",
    "    \n",
    "    chat_history.append(AIMessage(response))\n",
    "    chat_history = chat_history[-10:]\n",
    "    \n",
    "    return response\n",
    "\n",
    "submitUserMessage_debug(\"วิเคราะห์การเปิดร้านอาหารแถวนวลจันทร์\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: analyst\n",
      "Tool Calls:\n",
      "  search_population_community_household_expenditures_data (call_ay36WLn0KXh5bgoLJwuG8j6P)\n",
      " Call ID: call_ay36WLn0KXh5bgoLJwuG8j6P\n",
      "  Args:\n",
      "    query: นวลจันทร์\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_population_community_household_expenditures_data\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ยานนาวา\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ปทุมวัน\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: ทวีวัฒนา\n",
      ": 17\n",
      "\n",
      "จำนวนชุมชนประเภทต่าง ๆ ในกรุงเทพมหานคร พ.ศ. 2564: คันนายาว\n",
      ": 42\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: analyst\n",
      "\n",
      "%SIjfE923hf การตั้งราคาอาหารในร้านที่เปิดในนวลจันทร์ควรพิจารณาจากข้อมูลประชากรและชุมชนในพื้นที่ โดยในเขตคันนายาวมีจำนวนชุมชนถึง 42 ชุมชน ซึ่งแสดงถึงความหนาแน่นของประชากรที่อาจมีผลต่อกำลังซื้อของลูกค้า\n",
      "\n",
      "อย่างไรก็ตาม ข้อมูลเฉพาะเกี่ยวกับค่าใช้จ่ายของครัวเรือนในพื้นที่นวลจันทร์ยังไม่มีให้ในขณะนี้ ดังนั้นการตั้งราคาอาหารควรพิจารณาจากค่าใช้จ่ายทั่วไปในพื้นที่ใกล้เคียงและความสามารถในการแข่งขันของร้านอาหารในตลาด\n",
      "\n",
      "หากคุณมีข้อมูลเพิ่มเติมเกี่ยวกับประเภทอาหารที่ต้องการขายหรือกลุ่มเป้าหมายที่ต้องการเข้าถึง อาจช่วยให้การตั้งราคาเป็นไปได้อย่างเหมาะสมมากขึ้น\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' การตั้งราคาอาหารในร้านที่เปิดในนวลจันทร์ควรพิจารณาจากข้อมูลประชากรและชุมชนในพื้นที่ โดยในเขตคันนายาวมีจำนวนชุมชนถึง 42 ชุมชน ซึ่งแสดงถึงความหนาแน่นของประชากรที่อาจมีผลต่อกำลังซื้อของลูกค้า\\n\\nอย่างไรก็ตาม ข้อมูลเฉพาะเกี่ยวกับค่าใช้จ่ายของครัวเรือนในพื้นที่นวลจันทร์ยังไม่มีให้ในขณะนี้ ดังนั้นการตั้งราคาอาหารควรพิจารณาจากค่าใช้จ่ายทั่วไปในพื้นที่ใกล้เคียงและความสามารถในการแข่งขันของร้านอาหารในตลาด\\n\\nหากคุณมีข้อมูลเพิ่มเติมเกี่ยวกับประเภทอาหารที่ต้องการขายหรือกลุ่มเป้าหมายที่ต้องการเข้าถึง อาจช่วยให้การตั้งราคาเป็นไปได้อย่างเหมาะสมมากขึ้น'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submitUserMessage_debug(\"ราคาเหมาะสมที่ต้องขาย\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "def submitUserMessage(user_input: str, keep_chat_history:bool=True) -> str:\n",
    "    global chat_history\n",
    "    chat_history.append(HumanMessage(user_input))\n",
    "    \n",
    "    if not keep_chat_history:\n",
    "        chat_history = []\n",
    "    graph = workflow.compile()\n",
    "\n",
    "    events = graph.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    user_input\n",
    "                )\n",
    "            ],\n",
    "            \"chat_history\": chat_history\n",
    "        },\n",
    "        # Maximum number of steps to take in the graph\n",
    "        {\"recursion_limit\": 20},\n",
    "    )\n",
    "    \n",
    "    events = [e for e in events]\n",
    "    \n",
    "    response = list(events[-1].values())[0][\"messages\"][0]\n",
    "    response = response.content\n",
    "    response = response.replace(\"%SIjfE923hf\", \"\")\n",
    "    \n",
    "    chat_history.append(AIMessage(response))\n",
    "    chat_history = chat_history[-10:]\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# question = \"hello my frend\"\n",
    "# submitUserMessage(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
